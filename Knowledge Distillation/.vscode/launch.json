{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "llama minillm",
            "type": "python",
            "request": "launch",
            "program": "~/anaconda3/envs/kdllm/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES":"0",
                // "MASTER_ADDR": "localhost",
                // "MASTER_PORT": "2012",
                // "NNODES": "1",
                // "NODE_RANK": "0",
                // "GPUS_PER_NODE": "1",
                // "NCCL_DEBUG": "",
                // "WANDB_DISABLED": "True",
                // "TF_CPP_MIN_LOG_LEVEL": "3",
                // "PYTHONPATH": "/home/liuchao/shushanfu/LMOps",
                // "BASE_PATH": "/home/liuchao/shushanfu/LMOps"
            },
            "args": [
                "--nproc_per_node", "1",
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "2012",
                "${file}",
                "--base-path", "/home/liuchao/shushanfu/LMOps",
                "--model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/llama-7B/",
                "--teacher-model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/llama-13B/",
                "--ckpt-name", "llama-7B",
                "--teacher-ckpt-name", "13B-sft",
                "--n-gpu", "1",
                "--n-nodes", "1",
                "--model-type", "llama",
                "--teacher-model-fp16",
                "--gradient-checkpointing",
                "--prompt-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/dolly/prompt/llama/",
                "--lm-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/roberta/llama/512/20M/",
                // "--dev-num", "1000",
                "--num-workers", "0",
                "--epochs", "10",
                "--total-iters", "5000",
                "--kd-ratio", "0.5",
                "--batch-size", "8",
                "--lr", "5e-6",
                "--lr-min", "5e-6",
                "--gradient-accumulation-steps", "2",
                "--max-length", "512",
                "--max-prompt-length", "256",
                "--warmup-iters", "100",
                "--scheduler-name", "cosine_trm",
                "--save", "/home/liuchao/shushanfu/LMOps/results/llama/train/minillm/",
                "--seed", "10",
                "--seed-ppo", "42",
                "--seed-lm", "7",
                "--save-interval", "500",
                "--eval-interval", "100",
                "--log-interval", "16",
                "--mid-log-num", "1",
                "--peft", "lora",
                "--type", "minillm",
                "--ppo-epochs", "4",
                "--num-rollouts", "256",
                "--chunk-size", "8",
                "--length-norm",
                "--single-step-reg",
                "--teacher-mixed-alpha", "0.2",
                "--reward-scaling", "0.5",
                "--cliprange-reward", "100",
                "--do-sample",
                "--top-k", "0",
                "--top-p", "1.0",
                "--temperature", "1.0",
                "--deepspeed",
                "--deepspeed_config", "/home/liuchao/shushanfu/LMOps/configs/deepspeed/ds_config.json",
                
            ]
        },
        {
            "name": "codellama minillm",
            "type": "python",
            "request": "launch",
            "program": "~/anaconda3/envs/kdllm/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES":"0,2",
                // "MASTER_ADDR": "localhost",
                // "MASTER_PORT": "2012",
                // "NNODES": "1",
                // "NODE_RANK": "0",
                // "GPUS_PER_NODE": "1",
                // "NCCL_DEBUG": "",
                // "WANDB_DISABLED": "True",
                // "TF_CPP_MIN_LOG_LEVEL": "3",
                // "PYTHONPATH": "/home/liuchao/shushanfu/LMOps",
                // "BASE_PATH": "/home/liuchao/shushanfu/LMOps"
            },
            "args": [
                "--nproc_per_node", "2",
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "2012",
                "${file}",
                "--base-path", "/home/liuchao/shushanfu/LMOps",
                "--model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/llama-7B/",
                "--teacher-model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/llama-13B/",
                "--ckpt-name", "llama-7B",
                "--teacher-ckpt-name", "13B-sft",
                "--n-gpu", "1",
                "--n-nodes", "1",
                "--model-parallel",
                "--model-parallel-size", "2",
                "--model-type", "llama",
                "--teacher-model-fp16",
                "--gradient-checkpointing",
                "--prompt-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/dolly/prompt/llama/",
                "--lm-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/roberta/llama/512/20M/",
                // "--dev-num", "1000",
                "--num-workers", "0",
                "--epochs", "10",
                "--total-iters", "5000",
                "--kd-ratio", "0.5",
                "--batch-size", "8",
                "--lr", "5e-6",
                "--lr-min", "5e-6",
                "--gradient-accumulation-steps", "2",
                "--max-length", "512",
                "--max-prompt-length", "256",
                "--warmup-iters", "100",
                "--scheduler-name", "cosine_trm",
                "--save", "/home/liuchao/shushanfu/LMOps/results/llama/train/minillm/",
                "--seed", "10",
                "--seed-ppo", "42",
                "--seed-lm", "7",
                "--save-interval", "500",
                "--eval-interval", "100",
                "--log-interval", "16",
                "--mid-log-num", "1",
                "--peft", "lora",
                "--type", "minillm",
                "--ppo-epochs", "4",
                "--num-rollouts", "256",
                "--chunk-size", "8",
                "--length-norm",
                "--single-step-reg",
                "--teacher-mixed-alpha", "0.2",
                "--reward-scaling", "0.5",
                "--cliprange-reward", "100",
                "--do-sample",
                "--top-k", "0",
                "--top-p", "1.0",
                "--temperature", "1.0",
                "--deepspeed",
                "--deepspeed_config", "/home/liuchao/shushanfu/LMOps/configs/deepspeed/ds_config.json",
                
            ]
        },
        {
            "name": "Python: Train lora",
            "type": "python",
            "request": "launch",
            "program": "~/anaconda3/envs/kdllm/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES":"0",
            },
            "args": [
                "--nproc_per_node", "1",
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "2012",
                "${file}",
                "--base-path", "/home/liuchao/shushanfu/LMOps",
                "--model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/llama-7B/",
                "--ckpt-name", "llama-7B",
                "--n-gpu", "1",
                "--model-type", "llama",
                "--gradient-checkpointing",
                "--data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/dolly/full/llama/",
                "--num-workers", "0",
                // "--dev-num", "1000",
                "--lr", "0.0005",
                "--batch-size", "4",
                "--eval-batch-size", "8",
                "--gradient-accumulation-steps", "1",
                "--warmup-iters", "0",
                "--lr-decay-style", "cosine",
                "--weight-decay", "1e-2",
                "--clip-grad", "1.0",
                "--epochs", "10",
                "--max-length", "512",
                "--max-prompt-length", "256",
                "--do-train",
                "--do-valid",
                "--eval-gen",
                "--save-interval", "-1",
                "--eval-interval", "-1",
                "--log-interval", "4",
                "--mid-log-num", "1",
                "--save", "/home/liuchao/shushanfu/LMOps/results/llama/train/sft",
                "--peft", "lora",
                "--seed", "20",
                "--seed-order", "10",
                "--deepspeed",
                "--deepspeed_config", "/home/liuchao/shushanfu/LMOps/configs/deepspeed/ds_config_zero2.json",
                "--type", "lm",
                "--do-sample",
                "--top-k", "0",
                "--top-p", "1.0",
                "--temperature", "1.0",
            ]
        },
        {
            "name": "humaneval",
            "type": "python",
            "request": "launch",
            "program": "~/anaconda3/envs/kdllm/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES":"0",
            },
            "args": [
                "--master_port", "2024",
                "${file}",
            ]
        },
        {
            "name": "tiny minillama paralla",
            "type": "python",
            "request": "launch",
            "program": "~/anaconda3/envs/kdllm/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES":"0,2",
            },
            "args": [
                "--nproc_per_node", "2",
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "2024",
                "${file}",
                "--base-path", "/home/liuchao/shushanfu/LMOps",
                "--model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/TinyLlama-1.1B-python-v0.1/",
                "--teacher-model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/codellama-7B/",
                "--ckpt-name", "TinyLlama-1.1B-python-v0.1",
                "--teacher-ckpt-name", "7B-sft",
                "--n-gpu", "2",
                "--model-type", "llama",
                "--teacher-model-fp16",
                "--gradient-checkpointing",
                "--model-parallel",
                "--model-parallel-size", "2",
                "--prompt-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/codesearchnet/prompt/codellama/",
                "--lm-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/starcoderdata/codellama/512/20M/",
                "--num-workers", "0",
                "--epochs", "10",
                "--total-iters", "5000",
                "--kd-ratio", "0.5",
                "--batch-size", "2",
                "--lr", "5e-6",
                "--lr-min", "5e-6",
                "--gradient-accumulation-steps", "4",
                "--max-length", "512",
                "--max-prompt-length", "256",
                "--warmup-iters", "100",
                "--scheduler-name", "cosine_trm",
                "--save", "/home/liuchao/shushanfu/LMOps/results/codellama/train/minillm/",
                "--seed", "10",
                "--seed-ppo", "42",
                "--seed-lm", "7",
                "--save-interval", "500",
                "--eval-interval", "100",
                "--log-interval", "16",
                "--mid-log-num", "1",
                "--peft", "lora",
                "--do-train",
                "--type", "minillm",
                "--ppo-epochs", "4",
                "--num-rollouts", "256",
                "--chunk-size", "8",
                "--length-norm",
                "--single-step-reg",
                "--teacher-mixed-alpha", "0.2",
                "--reward-scaling", "0.5",
                "--cliprange-reward", "100",
                "--do-sample",
                "--top-k", "0",
                "--top-p", "1.0",
                "--temperature", "1.0",
                "--deepspeed",
                "--deepspeed_config", "/home/liuchao/shushanfu/LMOps/configs/deepspeed/ds_config_zero2.json"
            ],
        },
        {
            "name": "tiny minillama lora",
            "type": "python",
            "request": "launch",
            "program": "~/anaconda3/envs/kdllm/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES":"2",
            },
            "args": [
                "--nproc_per_node", "1",
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "2056",
                "${file}",
                "--base-path", "/home/liuchao/shushanfu/LMOps",
                "--model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/TinyLlama-1.1B-python-v0.1/",
                "--teacher-model-path", "/home/liuchao/shushanfu/LMOps/checkpoints/codellama-7B/",
                "--ckpt-name", "TinyLlama-1.1B-python-v0.1",
                "--teacher-ckpt-name", "7B-sft",
                "--n-gpu", "1",
                "--model-type", "llama",
                "--teacher-model-fp16",
                "--gradient-checkpointing",
                "--prompt-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/codesearchnet/prompt/codellama/",
                "--lm-data-dir", "/home/liuchao/shushanfu/LMOps/processed_data/starcoderdata/codellama/512/20M/",
                "--num-workers", "0",
                "--train-num", "20000",
                "--epochs", "10",
                "--total-iters", "5000",
                "--kd-ratio", "0.5",
                "--batch-size", "4",
                "--lr", "5e-6",
                "--lr-min", "5e-6",
                "--gradient-accumulation-steps", "4",
                "--max-length", "512",
                "--max-prompt-length", "256",
                "--warmup-iters", "100",
                "--scheduler-name", "cosine_trm",
                "--save", "/home/liuchao/shushanfu/LMOps/results/codellama/train/minillm/",
                "--seed", "10",
                "--seed-ppo", "42",
                "--seed-lm", "7",
                "--save-interval", "500",
                "--eval-interval", "100",
                "--log-interval", "16",
                "--mid-log-num", "1",
                "--peft", "org_lora",
                "--do-train",
                "--type", "minillm",
                "--ppo-epochs", "4",
                "--num-rollouts", "256",
                "--chunk-size", "8",
                "--length-norm",
                "--single-step-reg",
                "--teacher-mixed-alpha", "0.2",
                "--reward-scaling", "0.5",
                "--cliprange-reward", "100",
                "--do-sample",
                "--top-k", "0",
                "--top-p", "1.0",
                "--temperature", "1.0",
                "--deepspeed",
                "--deepspeed_config", "/home/liuchao/shushanfu/LMOps/configs/deepspeed/ds_config_zero2.json"
            ],
        },
        {
            "name": "debug_fintune",
            "type": "debugpy",
            "request": "attach",
            "justMyCode": false,
            "env": {
                "PYDEVD_WARN_EVALUATION_TIMEOUT":"1000",
            },
            "connect": {
                "host": "localhost",
                "port": 9502
            }
        },
        {
            "name": "debug myllm",
            "type": "python",
            "request": "launch",
            "program": "~/anaconda3/envs/kdllm/bin/torchrun",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES":"1",
                "DISPLAY":":1",
                "PYTHONPATH": "${workspaceRoot}",
                "PYDEVD_WARN_EVALUATION_TIMEOUT": "500"
            },
            "args": [
                "--nproc_per_node", "1",
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "2019",
                "${file}",
                "--base-path",
                "/home/liuchao/shushanfu/LMOps",
                "--model-path",
                "/home/liuchao/shushanfu/LMOps/checkpoints/TinyLlama-1.1B-python-v0.1/",
                "--teacher-model-path",
                "/home/liuchao/shushanfu/LMOps/checkpoints/codellama-7B/",
                "--ckpt-name",
                "TinyLlama-1.1B-python-v0.1",
                "--teacher-ckpt-name",
                "7B-sft",
                "--teacher-model-fp16",
                "--n-gpu",
                "1",
                "--model-type",
                "llama",
                "--gradient-checkpointing",
                "--data-dir",
                "/home/liuchao/shushanfu/LMOps/processed_data/codesearchnet/pseudo/codellama-7B-sft/",
                "--num-workers",
                "4",
                // "--train-num",
                // "200000",
                "--lr",
                "0.00001",
                "--batch-size",
                "8",
                "--eval-batch-size",
                "8",
                "--gradient-accumulation-steps",
                "1",
                "--warmup-iters",
                "0",
                "--lr-decay-style",
                "cosine",
                "--weight-decay",
                "1e-2",
                "--clip-grad",
                "1.0",
                "--epochs",
                "10",
                "--kd-ratio",
                "0.5",
                "--max-length",
                "512",
                "--max-prompt-length",
                "256",
                "--do-train",
                "--do-valid",
                "--eval-gen",
                "--save-interval",
                "-1",
                "--eval-interval",
                "-1",
                "--log-interval",
                "4",
                "--mid-log-num",
                "-1",
                "--save",
                "/home/liuchao/shushanfu/LMOps/results/codellama/train/seqkd",
                "--seed",
                "10",
                "--deepspeed",
                "--deepspeed_config",
                "/home/liuchao/shushanfu/LMOps/configs/deepspeed/ds_config.json",
                "--type",
                "kd",
                "--do-sample",
                "--top-k",
                "0",
                "--top-p",
                "1.0",
                "--temperature",
                "1.0"
            ],

        }


    ]
}
